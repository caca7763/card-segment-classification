import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from google.colab import drive
drive.mount('/content/drive')


# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('/content/drive/MyDrive/á„‡á…³á†¯á„…á…®á„†á…®á†«/2505_á„Žá…¥á†«á„‹á…¡á†«á„‰á…µ_á„Žá…©á„€á…³á†¸/á„ƒá…¦á„‹á…µá„á…¥/card_train.csv')  # íŒŒì¼ëª… ë³€ê²½ í•„ìš”


# ì‚¬ìš©í•  ë³€ìˆ˜ë§Œ ì„ íƒ
cols = [
    'ë‚¨ë…€êµ¬ë¶„ì½”ë“œ', 'ì—°ë ¹', 'ê±°ì£¼ì‹œë„ëª…', 'ì§ìž¥ì‹œë„ëª…',
    'ìž…íšŒê²½ê³¼ê°œì›”ìˆ˜_ì‹ ìš©', 'íƒˆíšŒíšŸìˆ˜_ëˆ„ì ', 'ìµœì¢…íƒˆíšŒí›„ê²½ê³¼ì›”',
    'ì†Œì§€ì—¬ë¶€_ì‹ ìš©', 'ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©',
    'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥', 'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA',
    'ë§ˆì¼€íŒ…ë™ì˜ì—¬ë¶€', 'Life_Stage', 'Segment'
]
df = df[cols].copy()


#íƒ€ê²Ÿê°’ ê°€ê³µ

df['target'] = df['Segment'].apply(lambda x: 1 if x == 'E' else 0)
df.drop(columns=['Segment'], inplace=True)


#ì „ì²˜ë¦¬

# ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
cat_cols = df.select_dtypes(include='object').columns

for col in cat_cols:
    df[col] = LabelEncoder().fit_transform(df[col].astype(str))


#ë¶„í• 

X = df.drop(columns='target')
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)


#ëª¨ë¸ í•™ìŠµ

rf = RandomForestClassifier(class_weight='balanced', random_state=42)
rf.fit(X_train, y_train)


#ëª¨ë¸ í‰ê°€

from sklearn.metrics import classification_report, confusion_matrix

# ì˜ˆì¸¡
y_pred = rf.predict(X_test)

# ì„±ëŠ¥ í‰ê°€
print("ðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred))

print("ðŸ§© Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))


from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder


#Non-E ë°ì´í„°ë§Œ ì¶”ì¶œ + íƒ€ê²Ÿ ë³‘í•©

# ì›ë³¸ CSV ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸° (2ë‹¨ê³„ ì‹œìž‘í•  ë•Œ!)
df_original = pd.read_csv('/content/drive/MyDrive/á„‡á…³á†¯á„…á…®á„†á…®á†«/2505_á„Žá…¥á†«á„‹á…¡á†«á„‰á…µ_á„Žá…©á„€á…³á†¸/á„ƒá…¦á„‹á…µá„á…¥/card_train.csv')  # â† ì‹¤ì œ íŒŒì¼ëª…ìœ¼ë¡œ ë°”ê¿”ì¤˜

# ì›ëž˜ Segmentì—ì„œ E ì œì™¸
df_multi = df_original[df_original['Segment'] != 'E'].copy()  # df_originalì€ ì „ì²´ CSVì—ì„œ ë¶ˆëŸ¬ì˜¨ ì›ë³¸

# íƒ€ê²Ÿ ë³‘í•©: A or B â†’ AB
df_multi['Segment_ABCD'] = df_multi['Segment'].apply(lambda x: 'AB' if x in ['A', 'B'] else x)

# ì‚¬ìš© ë³€ìˆ˜ ì„ íƒ
columns = [
    'ë‚¨ë…€êµ¬ë¶„ì½”ë“œ', 'ì—°ë ¹', 'ê±°ì£¼ì‹œë„ëª…', 'ì§ìž¥ì‹œë„ëª…',
    'ìž…íšŒê²½ê³¼ê°œì›”ìˆ˜_ì‹ ìš©', 'íƒˆíšŒíšŸìˆ˜_ëˆ„ì ', 'ìµœì¢…íƒˆíšŒí›„ê²½ê³¼ì›”',
    'ì†Œì§€ì—¬ë¶€_ì‹ ìš©', 'ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©',
    'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥', 'íšŒì›ì—¬ë¶€_ì´ìš©ê°€ëŠ¥_CA',
    'ë§ˆì¼€íŒ…ë™ì˜ì—¬ë¶€', 'Life_Stage', 'Segment_ABCD'
]
df_multi = df_multi[columns].copy()


#ë²”ì£¼í˜• ì¸ì½”ë”©

# Label Encoding
cat_cols = df_multi.select_dtypes(include='object').columns

for col in cat_cols:
    df_multi[col] = LabelEncoder().fit_transform(df_multi[col].astype(str))


#í•™ìŠµ,ê²€ì¦

X = df_multi.drop(columns='Segment_ABCD')
y = df_multi['Segment_ABCD']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)


#ëª¨ë¸í•™ìŠµ 3ê°œ

# ëª¨ë¸ 1: RandomForest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

# ëª¨ë¸ 2: LightGBM
lgb = LGBMClassifier(random_state=42)
lgb.fit(X_train, y_train)
lgb_pred = lgb.predict(X_test)

# ëª¨ë¸ 3: XGBoost
xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X_train, y_train)
xgb_pred = xgb.predict(X_test)


def evaluate_model(name, y_true, y_pred):
    print(f"\nðŸ“Š {name} ê²°ê³¼")
    print(classification_report(y_true, y_pred))
    print(confusion_matrix(y_true, y_pred))

evaluate_model("RandomForest", y_test, rf_pred)
evaluate_model("LightGBM", y_test, lgb_pred)
evaluate_model("XGBoost", y_test, xgb_pred)


#SMOTE ì ìš© ì‹œìž‘

!pip install imbalanced-learn catboost

from imblearn.over_sampling import SMOTE
from catboost import CatBoostClassifier


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# íƒ€ê²Ÿì€ Segment_ABCD (AB=1, C=2, D=0)
X = df_multi.drop(columns='Segment_ABCD')
y = df_multi['Segment_ABCD']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)


#SMOTE ì‹¤ì œ ì ìš©

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)


#ëª¨ë¸ë³„ ì ìš©

##RF

rf_sm = RandomForestClassifier(random_state=42)
rf_sm.fit(X_train_sm, y_train_sm)
rf_pred = rf_sm.predict(X_test)


##XGBC

xgb_sm = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')
xgb_sm.fit(X_train_sm, y_train_sm)
xgb_pred = xgb_sm.predict(X_test)


##CBC

cat_sm = CatBoostClassifier(verbose=0, random_seed=42)
cat_sm.fit(X_train_sm, y_train_sm)
cat_pred = cat_sm.predict(X_test)


#í‰ê°€í•¨ìˆ˜ ê³µí†µí™”

from sklearn.metrics import classification_report, confusion_matrix

def evaluate_model(name, y_true, y_pred):
    print(f"\nðŸ“Š {name} ê²°ê³¼")
    print(classification_report(y_true, y_pred))
    print(confusion_matrix(y_true, y_pred))

evaluate_model("SMOTE + RF", y_test, rf_pred)
evaluate_model("SMOTE + XGBoost", y_test, xgb_pred)
evaluate_model("SMOTE + CatBoost", y_test, cat_pred)


#D í´ëž˜ìŠ¤ ëª¨ë“  ëª¨ë¸ì—ì„œ ê±°ì˜ ë¯¸ë¯¸í•œ ì •ë„ ë”°ë¼ì„œ ì œê±° í›„ ABvsC ì´ì§„ ë¶„ë¥˜

# 0 = D ì œê±°
df_binary = df_multi[df_multi['Segment_ABCD'] != 0].copy()

# ìƒˆ íƒ€ê²Ÿ: AB=1, C=0
df_binary['target'] = df_binary['Segment_ABCD'].apply(lambda x: 1 if x == 1 else 0)
df_binary.drop(columns='Segment_ABCD', inplace=True)


##í•™ìŠµ/ê²€ì¦ ë¶„í•  + SMOTE

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

X = df_binary.drop(columns='target')
y = df_binary['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# SMOTEë¡œ í´ëž˜ìŠ¤ ê· í˜• ë§žì¶”ê¸°
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)


##CatBoostClassifier í•™ìŠµ ë° í‰ê°€(ê°€ìž¥ ìš°ìˆ˜í•œ ëª¨ë¸)

from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix

cat = CatBoostClassifier(verbose=0, random_seed=42)
cat.fit(X_train_sm, y_train_sm)
y_pred = cat.predict(X_test)

# ì„±ëŠ¥ í™•ì¸
print("ðŸ“Š CatBoost ê²°ê³¼ (AB vs C):")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))


# 1. ë‚˜ëˆ”í°íŠ¸ ì„¤ì¹˜
!apt-get update -qq
!apt-get install -y fonts-nanum

# 2. ëŸ°íƒ€ìž„ì— í°íŠ¸ ì ìš©
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

plt.rc('font', family='NanumGothic')  # í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€


feature_importance = cat.get_feature_importance(prettified=True)
print(feature_importance.head())


import matplotlib.pyplot as plt
import seaborn as sns

# ì¤‘ìš” ë³€ìˆ˜ ìƒìœ„ 10ê°œ ì‹œê°í™”
feature_importance = cat.get_feature_importance(prettified=True)
top_features = feature_importance.head(10)

plt.figure(figsize=(8, 6))
sns.barplot(data=top_features, x='Importances', y='Feature Id')
plt.title('Top 10 Feature Importance (CatBoost)')
plt.tight_layout()
plt.show()


from catboost import CatBoostClassifier

# íŠœë‹ëœ CatBoostClassifier
cat = CatBoostClassifier(
    iterations=600,        # ë” ì˜¤ëž˜ í•™ìŠµ
    depth=6,               # ë‚˜ë¬´ ê¹Šì´ (ë³µìž¡ë„ ì¡°ì ˆ)
    learning_rate=0.03,    # í•™ìŠµë¥  ë‚®ì¶°ì„œ ì•ˆì •ì ìœ¼ë¡œ
    l2_leaf_reg=5,         # ê³¼ì í•© ë°©ì§€ìš© L2 ì •ê·œí™”
    random_seed=42,
    verbose=100            # í•™ìŠµ ë¡œê·¸ ì¶œë ¥ (100íšŒ ë‹¨ìœ„)
)

# í•™ìŠµ
cat.fit(X_train_sm, y_train_sm)


from sklearn.metrics import classification_report, confusion_matrix

y_pred = cat.predict(X_test)

print("ðŸ“Š íŠœë‹ëœ CatBoost ê²°ê³¼:")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))


cat = CatBoostClassifier(
    iterations=600,
    depth=6,
    learning_rate=0.03,
    l2_leaf_reg=5,
    class_weights=[1, 1.5],  # [C í´ëž˜ìŠ¤, AB í´ëž˜ìŠ¤]
    random_seed=42,
    verbose=100
)
cat.fit(X_train_sm, y_train_sm)

y_pred = cat.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
print("ðŸ“Š CatBoost (ê°€ì¤‘ì¹˜ ì ìš©)")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))


#ì •í™•ë„ 70ì´ìƒìœ¼ë¡œ ì˜¬ë¦¬ê¸° ì‹œë„

from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix

cat = CatBoostClassifier(
    iterations=600,
    depth=6,
    learning_rate=0.03,
    l2_leaf_reg=5,
    random_seed=42,
    verbose=100
)

cat.fit(X_train_sm, y_train_sm)

y_pred = cat.predict(X_test)

print("ðŸ“Š CatBoost ê²°ê³¼ (ì •í™•ë„ ì¤‘ì‹¬ íŠœë‹)")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))


##ì •í™•ë„ ìƒìŠ¹ ìœ„í•´ íŒŒìƒë³€ìˆ˜ ì¶”ê°€

df['ìž…íšŒë‹¨ê³„'] = pd.cut(df['ìž…íšŒê²½ê³¼ê°œì›”ìˆ˜_ì‹ ìš©'], bins=[0,12,36,1000], labels=['ì‹ ê·œ','ì¤‘ê¸°','ìž¥ê¸°'])
df['ì¹´ë“œ3ì´ˆê³¼'] = (df['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'] > 3).astype(int)
df['ê±°ì£¼ì§ìž¥ì¼ì¹˜'] = (df['ê±°ì£¼ì‹œë„ëª…'] == df['ì§ìž¥ì‹œë„ëª…']).astype(int)


# ðŸ“Œ 1. ì›ë³¸ ë³µì‚¬
df_fe = df_binary.copy()

# ðŸ“Œ 2. íŒŒìƒë³€ìˆ˜ ìƒì„± (ðŸ’¥ ì—¬ê¸°ì—ì„œ ìˆ˜ì •!!)
# ðŸ”½ ê¸°ì¡´ ì½”ë“œì—ì„œ ì´ ë¶€ë¶„ì„ ì™„ì „ížˆ êµì²´í•˜ì„¸ìš” ðŸ”½

df_fe['ìž…íšŒë‹¨ê³„'] = pd.cut(
    df_fe['ìž…íšŒê²½ê³¼ê°œì›”ìˆ˜_ì‹ ìš©'],
    bins=[-1, 12, 36, df_fe['ìž…íšŒê²½ê³¼ê°œì›”ìˆ˜_ì‹ ìš©'].max()],
    labels=[0, 1, 2]  # ìˆ«ìž ë ˆì´ë¸” ì‚¬ìš©
).astype(int)  # ì •ìˆ˜í˜•ìœ¼ë¡œ ê°•ì œ ë³€í™˜ âœ…

# ðŸ“Œ 3. ë‹¤ë¥¸ íŒŒìƒë³€ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš© OK
df_fe['ì¹´ë“œ3ì´ˆê³¼'] = (df_fe['ì†Œì§€ì¹´ë“œìˆ˜_ìœ íš¨_ì‹ ìš©'] > 3).astype(int)
df_fe['ê±°ì£¼ì§ìž¥ì¼ì¹˜'] = (df_fe['ê±°ì£¼ì‹œë„ëª…'] == df_fe['ì§ìž¥ì‹œë„ëª…']).astype(int)

# ðŸ“Œ 4. Label Encoding
from sklearn.preprocessing import LabelEncoder

for col in df_fe.columns:
    if df_fe[col].dtype == 'object':
        df_fe[col] = LabelEncoder().fit_transform(df_fe[col].astype(str))

# ðŸ“Œ 5. SMOTE â†’ í•™ìŠµ
X = df_fe.drop(columns='target')
y = df_fe['target']

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)


X = df_fe.drop(columns='target')
y = df_fe['target']


##LightGBM í•™ìŠµ ë° í‰ê°€ ì½”ë“œ(íŒŒìƒë³€ìˆ˜ ì´í›„)

from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# 1. ë°ì´í„° ë¶„í• 
X = df_fe.drop(columns='target')
y = df_fe['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 2. SMOTE ì ìš©
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

# 3. LightGBM ëª¨ë¸ í•™ìŠµ
lgb = LGBMClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    random_state=42
)
lgb.fit(X_train_sm, y_train_sm)

# 4. ì˜ˆì¸¡ ë° í‰ê°€
y_pred = lgb.predict(X_test)

print("ðŸ“Š LightGBM ê²°ê³¼:")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))


import pandas as pd
import numpy as np

# íŒŒì¼ ì—…ë¡œë“œí–ˆì„ ê²½ìš° í•´ë‹¹ ê²½ë¡œë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
test_df = pd.read_csv('/content/drive/MyDrive/á„‡á…³á†¯á„…á…®á„†á…®á†«/2505_á„Žá…¥á†«á„‹á…¡á†«á„‰á…µ_á„Žá…©á„€á…³á†¸/á„ƒá…¦á„‹á…µá„á…¥/card_test.csv')

# ì˜ˆì‹œ: ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
if 'ê°€ìž…ì¼' in test_df.columns:
    test_df['ê°€ìž…ì¼'] = pd.to_datetime(test_df['ê°€ìž…ì¼'], errors='coerce')
    test_df['ê°€ìž…ì—°'] = test_df['ê°€ìž…ì¼'].dt.year
    test_df['ê°€ìž…ì›”'] = test_df['ê°€ìž…ì¼'].dt.month
    test_df['ê°€ìž…ì¼ìž'] = test_df['ê°€ìž…ì¼'].dt.day

# ì˜ˆì‹œ: íŒŒìƒ ë³€ìˆ˜ ìƒì„±
if 'ì´ìš©ê¸ˆì•¡' in test_df.columns:
    test_df['1íšŒì´ìš©ê¸ˆì•¡'] = test_df['ì´ìš©ê¸ˆì•¡'] / (test_df['ì´ìš©ê±´ìˆ˜'] + 1)

# ì˜ˆì‹œ: í•„ìš” ì—†ëŠ” ì»¬ëŸ¼ ì‚­ì œ
drop_cols = ['ê³ ê°ID', 'ê°€ìž…ì¼']
test_df = test_df.drop(columns=[col for col in drop_cols if col in test_df.columns], errors='ignore')


# ë²”ì£¼í˜• ë³€ìˆ˜ ë¼ë²¨ ì¸ì½”ë”© (ê°™ì€ ë°©ì‹ìœ¼ë¡œ trainê³¼ í†µì¼ë˜ì–´ì•¼ í•¨)
from sklearn.preprocessing import LabelEncoder

for col in test_df.select_dtypes(include='object').columns:
    test_df[col] = LabelEncoder().fit_transform(test_df[col].astype(str))


# ê°„ë‹¨ížˆ í‰ê· /ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´ (trainì—ì„œ í–ˆë˜ ë°©ì‹ ë”°ë¼ì•¼ í•¨)
for col in test_df.columns:
    if test_df[col].isnull().sum() > 0:
        if test_df[col].dtype == 'object':
            test_df[col] = test_df[col].fillna(test_df[col].mode()[0])
        else:
            test_df[col] = test_df[col].fillna(test_df[col].mean())


print("ì „ì²˜ë¦¬ ì™„ë£Œ âœ…")
print(test_df.shape)
test_df.head()


!pip install lightgbm
